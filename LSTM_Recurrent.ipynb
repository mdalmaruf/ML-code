{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  11295\n",
      "Total Vocab:  46\n",
      "Total Patterns:  11195\n",
      "Epoch 1/100\n",
      "11136/11195 [============================>.] - ETA: 1s - loss: 3.0888Epoch 00000: loss improved from inf to 3.08779, saving model to weights-improvement-00-3.0878-bigger.hdf5\n",
      "11195/11195 [==============================] - 287s - loss: 3.0878   \n",
      "Epoch 2/100\n",
      "11136/11195 [============================>.] - ETA: 1s - loss: 3.0424Epoch 00001: loss improved from 3.08779 to 3.04302, saving model to weights-improvement-01-3.0430-bigger.hdf5\n",
      "11195/11195 [==============================] - 291s - loss: 3.0430   \n",
      "Epoch 3/100\n",
      "11136/11195 [============================>.] - ETA: 1s - loss: 2.9956Epoch 00002: loss improved from 3.04302 to 2.99527, saving model to weights-improvement-02-2.9953-bigger.hdf5\n",
      "11195/11195 [==============================] - 291s - loss: 2.9953   \n",
      "Epoch 4/100\n",
      "11136/11195 [============================>.] - ETA: 1s - loss: 2.9177Epoch 00003: loss improved from 2.99527 to 2.91747, saving model to weights-improvement-03-2.9175-bigger.hdf5\n",
      "11195/11195 [==============================] - 299s - loss: 2.9175   \n",
      "Epoch 5/100\n",
      "11136/11195 [============================>.] - ETA: 1s - loss: 2.8541Epoch 00004: loss improved from 2.91747 to 2.85364, saving model to weights-improvement-04-2.8536-bigger.hdf5\n",
      "11195/11195 [==============================] - 301s - loss: 2.8536   \n",
      "Epoch 6/100\n",
      "11136/11195 [============================>.] - ETA: 1s - loss: 2.8097Epoch 00005: loss improved from 2.85364 to 2.80863, saving model to weights-improvement-05-2.8086-bigger.hdf5\n",
      "11195/11195 [==============================] - 300s - loss: 2.8086   \n",
      "Epoch 7/100\n",
      "11136/11195 [============================>.] - ETA: 1s - loss: 2.7739Epoch 00006: loss improved from 2.80863 to 2.77397, saving model to weights-improvement-06-2.7740-bigger.hdf5\n",
      "11195/11195 [==============================] - 299s - loss: 2.7740   \n",
      "Epoch 8/100\n",
      "11136/11195 [============================>.] - ETA: 1s - loss: 2.7541Epoch 00007: loss improved from 2.77397 to 2.75312, saving model to weights-improvement-07-2.7531-bigger.hdf5\n",
      "11195/11195 [==============================] - 300s - loss: 2.7531   \n",
      "Epoch 9/100\n",
      "11136/11195 [============================>.] - ETA: 1s - loss: 2.7256Epoch 00008: loss improved from 2.75312 to 2.72589, saving model to weights-improvement-08-2.7259-bigger.hdf5\n",
      "11195/11195 [==============================] - 299s - loss: 2.7259   \n",
      "Epoch 10/100\n",
      "11136/11195 [============================>.] - ETA: 1s - loss: 2.7062Epoch 00009: loss improved from 2.72589 to 2.70636, saving model to weights-improvement-09-2.7064-bigger.hdf5\n",
      "11195/11195 [==============================] - 301s - loss: 2.7064   \n",
      "Epoch 11/100\n",
      "11136/11195 [============================>.] - ETA: 1s - loss: 2.6740Epoch 00010: loss improved from 2.70636 to 2.67480, saving model to weights-improvement-10-2.6748-bigger.hdf5\n",
      "11195/11195 [==============================] - 301s - loss: 2.6748   \n",
      "Epoch 12/100\n",
      "11136/11195 [============================>.] - ETA: 1s - loss: 2.6505Epoch 00011: loss improved from 2.67480 to 2.64877, saving model to weights-improvement-11-2.6488-bigger.hdf5\n",
      "11195/11195 [==============================] - 303s - loss: 2.6488   \n",
      "Epoch 13/100\n",
      "11136/11195 [============================>.] - ETA: 1s - loss: 2.6235Epoch 00012: loss improved from 2.64877 to 2.62406, saving model to weights-improvement-12-2.6241-bigger.hdf5\n",
      "11195/11195 [==============================] - 303s - loss: 2.6241   \n",
      "Epoch 14/100\n",
      "11136/11195 [============================>.] - ETA: 1s - loss: 2.5836Epoch 00013: loss improved from 2.62406 to 2.58320, saving model to weights-improvement-13-2.5832-bigger.hdf5\n",
      "11195/11195 [==============================] - 312s - loss: 2.5832   \n",
      "Epoch 15/100\n",
      "11136/11195 [============================>.] - ETA: 1s - loss: 2.5510Epoch 00014: loss improved from 2.58320 to 2.55233, saving model to weights-improvement-14-2.5523-bigger.hdf5\n",
      "11195/11195 [==============================] - 310s - loss: 2.5523   \n",
      "Epoch 16/100\n",
      "11136/11195 [============================>.] - ETA: 1s - loss: 2.5142Epoch 00015: loss improved from 2.55233 to 2.51498, saving model to weights-improvement-15-2.5150-bigger.hdf5\n",
      "11195/11195 [==============================] - 300s - loss: 2.5150   \n",
      "Epoch 17/100\n",
      "11136/11195 [============================>.] - ETA: 1s - loss: 2.4755Epoch 00016: loss improved from 2.51498 to 2.47552, saving model to weights-improvement-16-2.4755-bigger.hdf5\n",
      "11195/11195 [==============================] - 327s - loss: 2.4755   \n",
      "Epoch 18/100\n",
      "11136/11195 [============================>.] - ETA: 1s - loss: 2.4269Epoch 00017: loss improved from 2.47552 to 2.42661, saving model to weights-improvement-17-2.4266-bigger.hdf5\n",
      "11195/11195 [==============================] - 305s - loss: 2.4266   \n",
      "Epoch 19/100\n",
      "11136/11195 [============================>.] - ETA: 1s - loss: 2.3796Epoch 00018: loss improved from 2.42661 to 2.37871, saving model to weights-improvement-18-2.3787-bigger.hdf5\n",
      "11195/11195 [==============================] - 313s - loss: 2.3787   \n",
      "Epoch 20/100\n",
      "11136/11195 [============================>.] - ETA: 1s - loss: 2.3197Epoch 00019: loss improved from 2.37871 to 2.31965, saving model to weights-improvement-19-2.3196-bigger.hdf5\n",
      "11195/11195 [==============================] - 295s - loss: 2.3196   \n",
      "Epoch 21/100\n",
      "11136/11195 [============================>.] - ETA: 1s - loss: 2.2621Epoch 00020: loss improved from 2.31965 to 2.26245, saving model to weights-improvement-20-2.2624-bigger.hdf5\n",
      "11195/11195 [==============================] - 302s - loss: 2.2624   \n",
      "Epoch 22/100\n",
      "11136/11195 [============================>.] - ETA: 1s - loss: 2.1941Epoch 00021: loss improved from 2.26245 to 2.19463, saving model to weights-improvement-21-2.1946-bigger.hdf5\n",
      "11195/11195 [==============================] - 300s - loss: 2.1946   \n",
      "Epoch 23/100\n",
      "11136/11195 [============================>.] - ETA: 1s - loss: 2.1154Epoch 00022: loss improved from 2.19463 to 2.11618, saving model to weights-improvement-22-2.1162-bigger.hdf5\n",
      "11195/11195 [==============================] - 323s - loss: 2.1162   \n",
      "Epoch 24/100\n",
      "11136/11195 [============================>.] - ETA: 1s - loss: 2.0574Epoch 00023: loss improved from 2.11618 to 2.05755, saving model to weights-improvement-23-2.0576-bigger.hdf5\n",
      "11195/11195 [==============================] - 315s - loss: 2.0576   \n",
      "Epoch 25/100\n",
      "11136/11195 [============================>.] - ETA: 13s - loss: 1.9781Epoch 00024: loss improved from 2.05755 to 1.97819, saving model to weights-improvement-24-1.9782-bigger.hdf5\n",
      "11195/11195 [==============================] - 2518s - loss: 1.9782   \n",
      "Epoch 26/100\n",
      "11136/11195 [============================>.] - ETA: 1s - loss: 1.9001Epoch 00025: loss improved from 1.97819 to 1.90090, saving model to weights-improvement-25-1.9009-bigger.hdf5\n",
      "11195/11195 [==============================] - 320s - loss: 1.9009   \n",
      "Epoch 27/100\n",
      "11136/11195 [============================>.] - ETA: 1s - loss: 1.8247Epoch 00026: loss improved from 1.90090 to 1.82484, saving model to weights-improvement-26-1.8248-bigger.hdf5\n",
      "11195/11195 [==============================] - 322s - loss: 1.8248   \n",
      "Epoch 28/100\n",
      "11136/11195 [============================>.] - ETA: 1s - loss: 1.7427Epoch 00027: loss improved from 1.82484 to 1.74311, saving model to weights-improvement-27-1.7431-bigger.hdf5\n",
      "11195/11195 [==============================] - 312s - loss: 1.7431   \n",
      "Epoch 29/100\n",
      "11136/11195 [============================>.] - ETA: 1s - loss: 1.6646Epoch 00028: loss improved from 1.74311 to 1.66495, saving model to weights-improvement-28-1.6650-bigger.hdf5\n",
      "11195/11195 [==============================] - 310s - loss: 1.6650   \n",
      "Epoch 30/100\n",
      "11136/11195 [============================>.] - ETA: 1s - loss: 1.5926Epoch 00029: loss improved from 1.66495 to 1.59342, saving model to weights-improvement-29-1.5934-bigger.hdf5\n",
      "11195/11195 [==============================] - 309s - loss: 1.5934   \n",
      "Epoch 31/100\n",
      "11136/11195 [============================>.] - ETA: 1s - loss: 1.5187Epoch 00030: loss improved from 1.59342 to 1.51870, saving model to weights-improvement-30-1.5187-bigger.hdf5\n",
      "11195/11195 [==============================] - 312s - loss: 1.5187   \n",
      "Epoch 32/100\n",
      "11136/11195 [============================>.] - ETA: 1s - loss: 1.4224Epoch 00031: loss improved from 1.51870 to 1.42294, saving model to weights-improvement-31-1.4229-bigger.hdf5\n",
      "11195/11195 [==============================] - 309s - loss: 1.4229   \n",
      "Epoch 33/100\n",
      "11136/11195 [============================>.] - ETA: 1s - loss: 1.3650Epoch 00032: loss improved from 1.42294 to 1.36504, saving model to weights-improvement-32-1.3650-bigger.hdf5\n",
      "11195/11195 [==============================] - 305s - loss: 1.3650   \n",
      "Epoch 34/100\n",
      "11136/11195 [============================>.] - ETA: 1s - loss: 1.2815Epoch 00033: loss improved from 1.36504 to 1.28267, saving model to weights-improvement-33-1.2827-bigger.hdf5\n",
      "11195/11195 [==============================] - 314s - loss: 1.2827   \n",
      "Epoch 35/100\n",
      "11136/11195 [============================>.] - ETA: 1s - loss: 1.2276Epoch 00034: loss improved from 1.28267 to 1.22740, saving model to weights-improvement-34-1.2274-bigger.hdf5\n",
      "11195/11195 [==============================] - 310s - loss: 1.2274   \n",
      "Epoch 36/100\n",
      "11136/11195 [============================>.] - ETA: 1s - loss: 1.1507Epoch 00035: loss improved from 1.22740 to 1.15031, saving model to weights-improvement-35-1.1503-bigger.hdf5\n",
      "11195/11195 [==============================] - 316s - loss: 1.1503   \n",
      "Epoch 37/100\n",
      "11136/11195 [============================>.] - ETA: 1s - loss: 1.0918Epoch 00036: loss improved from 1.15031 to 1.09119, saving model to weights-improvement-36-1.0912-bigger.hdf5\n",
      "11195/11195 [==============================] - 310s - loss: 1.0912   \n",
      "Epoch 38/100\n",
      "11136/11195 [============================>.] - ETA: 1s - loss: 1.0294Epoch 00037: loss improved from 1.09119 to 1.03021, saving model to weights-improvement-37-1.0302-bigger.hdf5\n",
      "11195/11195 [==============================] - 311s - loss: 1.0302   \n",
      "Epoch 39/100\n",
      "11136/11195 [============================>.] - ETA: 1s - loss: 0.9670Epoch 00038: loss improved from 1.03021 to 0.96805, saving model to weights-improvement-38-0.9680-bigger.hdf5\n",
      "11195/11195 [==============================] - 317s - loss: 0.9680   \n",
      "Epoch 40/100\n",
      "11136/11195 [============================>.] - ETA: 1s - loss: 0.9286Epoch 00039: loss improved from 0.96805 to 0.92841, saving model to weights-improvement-39-0.9284-bigger.hdf5\n",
      "11195/11195 [==============================] - 308s - loss: 0.9284   \n",
      "Epoch 41/100\n",
      "11136/11195 [============================>.] - ETA: 1s - loss: 0.8568Epoch 00040: loss improved from 0.92841 to 0.85579, saving model to weights-improvement-40-0.8558-bigger.hdf5\n",
      "11195/11195 [==============================] - 303s - loss: 0.8558   \n",
      "Epoch 42/100\n",
      "11136/11195 [============================>.] - ETA: 1s - loss: 0.8152Epoch 00041: loss improved from 0.85579 to 0.81577, saving model to weights-improvement-41-0.8158-bigger.hdf5\n",
      "11195/11195 [==============================] - 314s - loss: 0.8158   \n",
      "Epoch 43/100\n",
      "11136/11195 [============================>.] - ETA: 1s - loss: 0.7591Epoch 00042: loss improved from 0.81577 to 0.75859, saving model to weights-improvement-42-0.7586-bigger.hdf5\n",
      "11195/11195 [==============================] - 327s - loss: 0.7586   \n",
      "Epoch 44/100\n",
      "11136/11195 [============================>.] - ETA: 1s - loss: 0.7190Epoch 00043: loss improved from 0.75859 to 0.71862, saving model to weights-improvement-43-0.7186-bigger.hdf5\n",
      "11195/11195 [==============================] - 313s - loss: 0.7186   \n",
      "Epoch 45/100\n",
      "11136/11195 [============================>.] - ETA: 1s - loss: 0.6731Epoch 00044: loss improved from 0.71862 to 0.67377, saving model to weights-improvement-44-0.6738-bigger.hdf5\n",
      "11195/11195 [==============================] - 329s - loss: 0.6738   \n",
      "Epoch 46/100\n",
      "11136/11195 [============================>.] - ETA: 1s - loss: 0.6408Epoch 00045: loss improved from 0.67377 to 0.64105, saving model to weights-improvement-45-0.6410-bigger.hdf5\n",
      "11195/11195 [==============================] - 314s - loss: 0.6410   \n",
      "Epoch 47/100\n",
      "11136/11195 [============================>.] - ETA: 1s - loss: 0.5921Epoch 00046: loss improved from 0.64105 to 0.59200, saving model to weights-improvement-46-0.5920-bigger.hdf5\n",
      "11195/11195 [==============================] - 318s - loss: 0.5920   \n",
      "Epoch 48/100\n",
      "11136/11195 [============================>.] - ETA: 1s - loss: 0.5584Epoch 00047: loss improved from 0.59200 to 0.55763, saving model to weights-improvement-47-0.5576-bigger.hdf5\n",
      "11195/11195 [==============================] - 319s - loss: 0.5576   \n",
      "Epoch 49/100\n",
      "11136/11195 [============================>.] - ETA: 1s - loss: 0.5326Epoch 00048: loss improved from 0.55763 to 0.53287, saving model to weights-improvement-48-0.5329-bigger.hdf5\n",
      "11195/11195 [==============================] - 326s - loss: 0.5329   \n",
      "Epoch 50/100\n",
      "11136/11195 [============================>.] - ETA: 1s - loss: 0.4834Epoch 00049: loss improved from 0.53287 to 0.48340, saving model to weights-improvement-49-0.4834-bigger.hdf5\n",
      "11195/11195 [==============================] - 319s - loss: 0.4834   \n",
      "Epoch 51/100\n",
      "11136/11195 [============================>.] - ETA: 1s - loss: 0.4697Epoch 00050: loss improved from 0.48340 to 0.46962, saving model to weights-improvement-50-0.4696-bigger.hdf5\n",
      "11195/11195 [==============================] - 330s - loss: 0.4696   \n",
      "Epoch 52/100\n",
      "11136/11195 [============================>.] - ETA: 1s - loss: 0.4374Epoch 00051: loss improved from 0.46962 to 0.43703, saving model to weights-improvement-51-0.4370-bigger.hdf5\n",
      "11195/11195 [==============================] - 324s - loss: 0.4370   \n",
      "Epoch 53/100\n",
      "11136/11195 [============================>.] - ETA: 1s - loss: 0.4219Epoch 00052: loss improved from 0.43703 to 0.42174, saving model to weights-improvement-52-0.4217-bigger.hdf5\n",
      "11195/11195 [==============================] - 304s - loss: 0.4217   \n",
      "Epoch 54/100\n",
      "11136/11195 [============================>.] - ETA: 1s - loss: 0.4059Epoch 00053: loss improved from 0.42174 to 0.40562, saving model to weights-improvement-53-0.4056-bigger.hdf5\n",
      "11195/11195 [==============================] - 323s - loss: 0.4056   \n",
      "Epoch 55/100\n",
      "11136/11195 [============================>.] - ETA: 1s - loss: 0.3683Epoch 00054: loss improved from 0.40562 to 0.36868, saving model to weights-improvement-54-0.3687-bigger.hdf5\n",
      "11195/11195 [==============================] - 292s - loss: 0.3687   \n",
      "Epoch 56/100\n",
      " 7104/11195 [==================>...........] - ETA: 108s - loss: 0.3342"
     ]
    }
   ],
   "source": [
    "# Larger LSTM Network to Generate Text for Alice in Wonderland\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "# load ascii text and covert to lowercase\n",
    "filename = \"wonderland.txt\"\n",
    "raw_text = open(filename,  encoding=\"utf8\").read()\n",
    "raw_text = raw_text.lower()\n",
    "# create mapping of unique chars to integers\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "# summarize the loaded data\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print (\"Total Characters: \", n_chars)\n",
    "print (\"Total Vocab: \", n_vocab)\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print (\"Total Patterns: \", n_patterns)\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)\n",
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "# fit the model\n",
    "history= model.fit(X, y, epochs= 100, batch_size=64, callbacks=callbacks_list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Load Larger LSTM network and generate text\n",
    "import sys\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11195, 45)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load ascii text and covert to lowercase\n",
    "filename = \"wonderland.txt\"\n",
    "raw_text = open(filename, encoding=\"utf8\").read()\n",
    "raw_text = raw_text.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  11295\n",
      "Total Vocab:  46\n"
     ]
    }
   ],
   "source": [
    "# create mapping of unique chars to integers, and a reverse mapping\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "# summarize the loaded data\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print (\"Total Characters: \", n_chars)\n",
    "print (\"Total Vocab: \", n_vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  11265\n"
     ]
    }
   ],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 30\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print (\"Total Patterns: \", n_patterns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11265, 30, 1)\n",
      "(11265, 45)\n"
     ]
    }
   ],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "482\n",
      "Input Data:\n",
      "\"  christmas.\n",
      "\n",
      "there was clearly \"\n"
     ]
    }
   ],
   "source": [
    "# load the network weights\n",
    "#filename = \"weights-improvement-19-2.3398-bigger.hdf5\"\n",
    "#filename = \"weights-improvement-49-0.6174-bigger.hdf5\"\n",
    "filename = \"weights-improvement-54-0.3687-bigger.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "# pick a random seed\n",
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "print(start)\n",
    "\n",
    "pattern = dataX[start]\n",
    "print (\"Input Data:\")\n",
    "print (\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Sentence\n",
      " nothing to doy jim. sometning fine and racee for mim a pingne and stood becore to toe wur ous aglis"
     ]
    }
   ],
   "source": [
    "# generate characters\n",
    "print('Generated Sentence')\n",
    "for i in range(100):\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-74752e01f3bf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'keras_model.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"history.p\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"wb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'keras_model.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"history.p\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "model.save('keras_model.h5')\n",
    "pickle.dump(history, open(\"history.p\", \"wb\"))\n",
    "model = load_model('keras_model.h5')\n",
    "history = pickle.load(open(\"history.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-3918201b9277>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'model accuracy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'epoch'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'test'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'upper left'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'callbacks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-bc15785ae54e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcallbacks\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'callbacks' is not defined"
     ]
    }
   ],
   "source": [
    "callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
